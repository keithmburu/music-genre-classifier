    # N_k_counts = {}
        # N_kjv_counts = {}
        # print("\ncounting N_k, N_kjv\n")
        # for example in train_partition.data:
        #     if example.label in N_k_counts:
        #         N_k_counts[example.label] += 1
        #     else:
        #         N_k_counts[example.label] = 1
        #     for feature in example.features:
        #         if feature in self.feature_names:
        #             if example.label in N_kjv_counts:
        #                 if feature in N_kjv_counts[example.label]:
        #                     if example.features[feature] in N_kjv_counts[example.label][feature]:
        #                         N_kjv_counts[example.label][feature][example.features[feature]] += 1
        #                     else:
        #                         N_kjv_counts[example.label][feature][example.features[feature]] = 1
        #                         if example.features[feature] == "True":
        #                             N_kjv_counts[example.label][feature]["False"] = 0
        #                         else:
        #                             N_kjv_counts[example.label][feature]["True"] = 0
        #                 else:
        #                     N_kjv_counts[example.label][feature] = {}
        #                     N_kjv_counts[example.label][feature][example.features[feature]] = 1
        #                     if example.features[feature] == "True":
        #                         N_kjv_counts[example.label][feature]["False"] = 0
        #                     else:
        #                         N_kjv_counts[example.label][feature]["True"] = 0
        #             else:
        #                 N_kjv_counts[example.label] = {}
        #                 N_kjv_counts[example.label][feature] = {}
        #                 N_kjv_counts[example.label][feature][example.features[feature]] = 1
        #                 if example.features[feature] == "True":
        #                     N_kjv_counts[example.label][feature]["False"] = 0
        #                 else:
        #                     N_kjv_counts[example.label][feature]["True"] = 0

        # # print(N_k_counts, N_kjv_counts[5])

        # self.prior = [math.log(N_k_counts[label] + 1) - math.log(train_partition.n + K) for label in N_k_counts.keys()]
        # self.likelihood = []
        # print("\nconstructing likelihood\n")
        # for label in N_kjv_counts:
        #     print(f"\nlabel {label}\n")
        #     label_dict = {}
        #     for feature in N_kjv_counts[label]:
        #         feature_dict = {}
        #         for value in N_kjv_counts[label][feature]:
        #             feature_dict[value] = math.log(N_kjv_counts[label][feature][value] + 1) - math.log(N_k_counts[label] + 
        #                     len(N_kjv_counts[label][feature]))
        #         # print(feature_dict)
        #         label_dict[feature] = feature_dict
        #     self.likelihood.append(label_dict)

# def confusion_matrix_formatted(confusion_matrix):
#     """
#     Converts confusion matrix to a more readable format
#     confusion_matrix: array showing classification accuracy
#     """
#     output = "      prediction\n     0  1  2  3  4  5  6  7  8  9\n   ---------------\n 0|"+\
#         "  {a}  {b} \n 1|  {k}  {l} \n 2|  {k}  {l} ".format(i=confusion_matrix[0][0],
#         j=confusion_matrix[0][1], k=confusion_matrix[1][0],
#         l=confusion_matrix[1][1])
#     return output

    # data = pd.DataFrame(pd.read_csv("data/features_n20_3_sec_shuffled.csv"))
    # print(data)
    # sns.pairplot(data, hue="label")
    # exit()

    # features being considered
    feature_names = relevant_features(train_file, train_partition.F.keys(), percentage_of_features)

    # initializing model
    clf = MultinomialNB()
    print("\nmodel initialized\n")
    X = np.zeros([train_partition.n, len(train_partition.F)])
    y = np.zeros([train_partition.n, 1])
    for i, example in enumerate(train_partition.data):
        print(f"\ninitializing train X and y, {i}\n")
        j = 0
        for feature in example.features:
            if feature in feature_names:
                if example.features[feature] == "True":  
                    X[i, j] = 1
                j += 1
        y[i] = example.label
    print(y)

    print("\nfitting model\n")
    clf.fit(X, y)
    
    real_values = []
    X_0 = np.zeros([test_partition_refeaturized.n, len(test_partition_refeaturized.F)])
    # classifying examples
    for i, example in enumerate(test_partition_refeaturized.data):
        print(f"classifying, {i}")
        real_values.append(example.label)
        for j, feature_val in enumerate(example.features.values()):
            if feature_val == "True":  
                X_0[i, j] = 1
    print(X_0)
    model_values = clf.predict(X_0) 
    print(model_values)

    # presenting accuracy and confusion matrix
    cm = confusion_matrix(train_partition.K, real_values, model_values)
    print("\nAccuracy: " + str(accuracy(cm, len(real_values))) + "(" + str(accuracy(cm, len(real_values)) * \
    test_partition_refeaturized.n) + " out of " + str(test_partition_refeaturized.n) + " correct)")
    print(cm)


nb n80 100% cont 100% disc
Accuracy: 0.8181818181818182(18.0 out of 22 correct)
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 2. 0. 0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 1. 0. 0. 0. 2. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 3. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 6. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]

nb n80 100% cont 75% disc
 Accuracy: 0.9090909090909091(20.0 out of 22 correct)
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 1. 2. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 3. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 6. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]

nb n80 100% cont 50% disc
Accuracy: 0.8636363636363636(19.0 out of 22 correct)
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 3. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 6. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]

nb n80 100% cont 10% disc
Accuracy: 0.8636363636363636(19.0 out of 22 correct)
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 0. 0. 0. 0. 2.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 3. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 5. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]

nb n80 100% cont 1% disc
 Accuracy: 0.4090909090909091(9.0 out of 22 correct)
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 2. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 2.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 3. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]
 [0. 2. 0. 0. 1. 0. 0. 1. 2. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]

nb n800 100% cont 1% disc
 Accuracy: 0.19402985074626866(39.0 out of 201 correct)
[[ 4.  4.  7.  1.  0.  0.  1.  0.  2.  0.]
 [ 5.  0.  6.  0.  0.  0.  0.  1.  0.  2.]
 [ 3.  4.  4.  0.  0.  0.  1.  0.  0.  0.]
 [ 3.  0.  0.  7.  3.  0.  5.  2.  2.  1.]
 [ 0.  0.  0. 10.  0.  0.  6.  0.  5.  0.]
 [ 2.  3.  6.  0.  0.  1.  2.  3.  0.  0.]
 [ 0.  0.  0.  5.  0.  0.  8.  0.  2.  1.]
 [ 0.  0.  2. 12.  0.  0.  4.  7.  0.  1.]
 [ 2.  0.  5.  7.  1.  0.  7.  1.  6.  4.]
 [ 1.  1.  3.  4.  1.  2.  2.  2.  2.  2.]]

 """
CS260 Project
Categorical Naive Bayes from scratch and using sklearn's implementation
Author: Keith Mburu
Author: Matt Gusdorff
Date: 12/17/2021
"""

import os
import math
from collections import OrderedDict
from numpy.lib.type_check import real
import util
import numpy as np
from entropy import Entropy
import cont_to_disc
import matplotlib.pyplot as plt 
import seaborn as sns
import sklearn.naive_bayes as nb
import pandas as pd

class NaiveBayes:

    def __init__(self, train_partition, train_file, percentage_of_features=100):
        """
        Initializes model that is used to classify examples
        train_partition: object containing training data for model creation
        best_features_only: bool indicating whether we only want best features
        """
        # dictionary of features and possible feature values
        F = train_partition.F
        # number of classes
        K = train_partition.K
        # features being considered
        # sorted features
        self.feature_names = relevant_features(train_file, F.keys(), percentage_of_features)

        # unsorted features
        # features = list(F.keys())
        # random.shuffle(features)
        # self.feature_names = features[:math.ceil((percentage_of_features/100)*len(F.keys()))]

        if os.path.exists("objects/" + "nb_" + str(percentage_of_features) + "%_" + train_file + ".pkl"):
            print("\nloading prior,likelihood\n")
            self.prior, self.likelihood = util.object_load("nb_" + str(percentage_of_features) + "%_" + train_file)     
        elif os.path.exists("objects/" + "nb_100%_" + train_file + ".pkl"):
            print("\nloading prior,likelihood\n")
            self.prior, self.likelihood = util.object_load("nb_100%_" + train_file)
        else:
            # list containing prior p(y=k) for each class k
            self.prior = [0] * K
            # list of k dictionaries corresponding to k classes, each with j
            # dictionaries corresponding to j features, and containing the
            # likelihood of each feature value p(x_v | y=k)
            self.likelihood = [{}] * K
        
            for k in range(K):
                # number of examples of class k
                N_k = 0
                print("\nconstructing prior\n")
                for example in train_partition.data:
                    if example.label == k:
                        N_k += 1
                # updating prior p(y=k)
                self.prior[k] = math.log(N_k + 1) - math.log(train_partition.n + K)

                # dictionary corresponding to y=k
                y_k_dict = OrderedDict()
                print("\nconstructing likelihood\n")
                for j in range(len(self.feature_names)):
                    # dictionary corresponding to feature j
                    feature_value_dict = OrderedDict()
                    # for each possible feature value
                    for v in range(len(F[self.feature_names[j]])):
                        # number of examples of class k that have feature j=v
                        N_kjv = 0
                        for example in train_partition.data:
                            if example.label == k and \
                            example.features[self.feature_names[j]] == \
                            F[self.feature_names[j]][v]:
                                N_kjv += 1
                        # updating likelihood p(x_v | y=k)
                        feature_value_dict[F[self.feature_names[j]][v]] = \
                            math.log(N_kjv + 1) - math.log(N_k + \
                                len(F[self.feature_names[j]]))
                    y_k_dict[self.feature_names[j]] = feature_value_dict
                self.likelihood[k] = y_k_dict
            print("\nstoring prior, likelihood\n")
            util.object_store([self.prior, self.likelihood], "nb_" + str(percentage_of_features) + "%_" + train_file)

    def classify(self, x_test):
        """
        Predicts the class of an example based on its feature real_values
        x_test: example's dictionary of features
        """
        # predicted label (default = -1)
        k_hat = -1
        # initializing arbitrary small log posterior probability
        y_hat = -1000000
        for k in range(len(self.prior)):
            y_k_hat = self.prior[k]
            for feature in self.feature_names:
                # log addition
                y_k_hat += self.likelihood[k][feature][x_test[feature]]
            # finding class with max probability
            if y_k_hat > y_hat:
                k_hat = k
                y_hat = y_k_hat
        return k_hat

def relevant_features(train_file, all_features, percentage_of_features):
        if percentage_of_features != 100:
            print("\nloading sorted features\n")
            tokens = train_file.split("_")
            n = tokens[3][1:]
            try:
                sorted_features = util.object_load(f"100%_disc_features_sorted_n{n}")
            except:
                print(f"creating sorted features list for n={n}")
                try:
                    full_partition = util.object_load(f"1_100%_features_n{n}_3_sec_shuffled.csv")
                except:
                    print(f"read in csv file with 100% of the features for n={n} first")
                    exit()
                sorted_features = Entropy(full_partition).sort_features()
                print("\nstoring sorted features\n")
                util.object_store(sorted_features, f"100%_disc_features_sorted_n{n}")
            return sorted_features[:math.ceil((percentage_of_features/100)*len(sorted_features))]
        else:
            return list(all_features)
            
def confusion_matrix(K, real_values, model_values):
    """
    Compares real values to prediction values to come up with true positives
    and negatives, false positive and negatives for the confusion matrix
    real_values: actual labels
    model_values: predicted labels
    """
    confusion_matrix = np.zeros([K, K])
    # updating confusion matrix metrics
    for i in range(len(model_values)):
        confusion_matrix[real_values[i], int(model_values[i])] += 1
    return confusion_matrix

def accuracy(confusion_matrix, total):
    """
    Measures proportion of examples that were classified correctly
    confusion_matrix: 2-D array showing true positives and negatives, false
    positives and negatives
    """
    correct = 0
    for i in range(10):
        correct += confusion_matrix[i][i]
    return correct / total

def testing():



def main():
    n = 800
    disc = 1
    genre = "disco"
    n_2 = 200
    train_filename = f"data/features_n{n}_3_sec_shuffled.csv"
    test_filename = f"data/{genre}_tests_n{n_2}.csv"
    # test_filename = f"data/features_n{int(n/4)}_3_sec_shuffled.csv"
    train_file, train_partition = util.read_csv(train_filename, disc, 100)
    test_file, test_partition = util.read_csv(test_filename, 0, 100)

    if disc:
        test_partition_refeaturized = cont_to_disc.refeaturize(train_partition, test_partition, test_file)
    else:
        test_partition_refeaturized = test_partition

    x_vals = []
    y_vals = []
    for percentage in range(10, 101, 10):
        print(f"\nPercentage: {percentage}\n")
        # initializing model
        nb_model = NaiveBayes(train_partition, train_file, percentage)
        print("\nmodel initialized\n")
        real_values = []
        model_values = []
        print("classifying")
        for example in test_partition_refeaturized.data:
            real_values.append(example.label)
            # classifying examples
            model_values.append(nb_model.classify(example.features))
        # presenting accuracy and confusion matrix
        cm = confusion_matrix(train_partition.K, real_values, model_values)
        print("\nAccuracy: " + str(accuracy(cm, len(real_values))) + "(" + str(accuracy(cm, len(real_values)) * \
        test_partition_refeaturized.n) + " out of " + str(test_partition_refeaturized.n) + " correct)")
        print(cm)

        x_vals.append(percentage)
        y_vals.append(accuracy(cm, len(real_values))*100)
        
        arr = np.array(cm)
        cm_df = pd.DataFrame(arr, index = util.GENRES, columns = util.GENRES)
        plt.figure(figsize = (7,6))
        sns.heatmap(cm_df, annot=True, cmap="Greens", fmt='g')
        plt.savefig(f"figures/nb_n{n}_{genre}_{percentage}%_cm.png")
        plt.clf()

    plt.plot(x_vals, y_vals)
    plt.xlim([10, 100])
    plt.ylim([0, 100])
    plt.title(f"Naive Bayes {genre} accuracy vs percentage of features used")
    plt.xlabel("Percentage of features used")
    plt.ylabel("Classification accuracy")
    plt.savefig(f"figures/nb_n{n}_{genre}_accuracy.png")
    plt.show()
    plt.clf()

    #sklearn implementation

    x_vals = []
    y_vals = []
    for percentage in range(1, 42, 10):
        print(f"\nPercentage: {percentage}%\n")
        # features being considered
        feature_names = relevant_features(train_file, train_partition.F.keys(), percentage)
        # initializing model
        clf = nb.CategoricalNB()
        print("\nmodel initialized\n")
        if os.path.exists(f"objects/X_y_{disc}_{percentage}%_{train_file}.pkl"):
            print("\nloading X, y\n")
            X, y = util.object_load(f"X_y_{disc}_{percentage}%_{train_file}")
        else:  
            X = np.zeros([train_partition.n, len(feature_names)])
            y = np.zeros([train_partition.n, 1])
            for i, example in enumerate(train_partition.data):
                print(f"\ninitializing train X and y, {i}\n")
                j = 0
                for feature in example.features:
                    if feature in feature_names:
                        if disc:
                            if example.features[feature] == "True":  
                                X[i, j] = 1
                        else:
                            X[i, j] = example.features[feature]
                        j += 1
                y[i] = example.label
            util.object_store([X, y], f"X_y_{disc}_{percentage}%_{train_file}")

        print("\nfitting model\n")
        clf.fit(X, y)
        
        real_values = []
        if os.path.exists(f"objects/X0_y0_0_{percentage}%_{test_file}.pkl"):
            print("\nloading X0, y0\n")
            X_0, y_0 = util.object_load(f"X0_y0_0_{percentage}%_{test_file}")
        else:  
            X_0 = np.zeros([test_partition_refeaturized.n, len(feature_names)])
            y_0 = np.zeros([test_partition_refeaturized.n, 1])
            # classifying examples
            for i, example in enumerate(test_partition_refeaturized.data):
                print(f"\ninitializing test X0 and y0, {i}\n")
                j = 0
                for feature in example.features:
                    if feature in feature_names:
                        if disc:
                            if example.features[feature] == "True":  
                                X_0[i, j] = 1
                        else:
                            X_0[i, j] = example.features[feature]
                        j += 1
                y_0[i] = example.label
                real_values.append(example.label)
            util.object_store([X_0, y_0], f"X0_y0_0_{percentage}%_{test_file}")

        # print(X_0)
        model_values = clf.predict(X_0) 
        # print(model_values)

        # presenting accuracy and confusion matrix
        cm = confusion_matrix(train_partition.K, real_values, model_values)
        print("\nAccuracy: " + str(accuracy(cm, len(real_values))) + "(" + str(accuracy(cm, len(real_values)) * \
        test_partition_refeaturized.n) + " out of " + str(test_partition_refeaturized.n) + " correct)")
        print(cm)

        arr = np.array(cm)
        cm_df = pd.DataFrame(arr, index = util.GENRES, columns = util.GENRES)
        plt.figure(figsize = (7,6))
        sns.heatmap(cm_df, annot=True, cmap="Greens")
        plt.savefig(f"figures/nb(sklearn)_n{n}_{percentage}_cm.png")
        plt.show()

        x_vals.append(percentage)
        y_vals.append(accuracy(cm, len(real_values))*100)
    plt.plot(x_vals, y_vals)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.title(f"Naive Bayes sklearn accuracy vs percentage of features used")
    plt.xlabel("Percentage of features used")
    plt.ylabel("Classification accuracy")
    plt.savefig(f"figures/nb(sklearn)_n{n}_accuracy.pdf")
    plt.show()
    plt.clf()

if __name__ == "__main__":
    main()

def confusion_matrix(K, real_values, model_values):
    """
    Compares real values to prediction values
    real_values: actual labels
    model_values: predicted labels
    """
    confusion_matrix = np.zeros([K, K])
    # updating confusion matrix metrics
    for i in range(len(model_values)):
        print(i)
        confusion_matrix[real_values[i], int(model_values[i])] += 1
    return confusion_matrix

def accuracy(confusion_matrix, total):
    """
    Measures proportion of examples that were classified correctly
    confusion_matrix: 2-D array showing true positives and negatives, false
    positives and negatives
    """
    correct = 0
    for i in range(10):
        correct += confusion_matrix[i][i]
    return correct / total